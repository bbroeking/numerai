{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExample classifier on Numerai data using a xgboost regression.\\nTo get started, install the required packages: pip install pandas numpy sklearn xgboost\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Example classifier on Numerai data using a xgboost regression.\n",
    "To get started, install the required packages: pip install pandas numpy sklearn xgboost\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from models.burningcrusade.data_preparation import prepare_data\n",
    "from helpers.utils import generate_features_list, load_model, clean_for_xgboost\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numerapi\n",
    "NAPI = numerapi.NumerAPI(verbosity=\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_NAME = f\"target\"\n",
    "PREDICTION_NAME = f\"prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = \"../models/burningcrusade/\"\n",
    "QURTY_MODEL = Path(pre+\"burningcrusade_green_mmc_good_corr.pkl\")\n",
    "BOOMKIN_MODEL = Path(pre+\"wotlk.pkl\")\n",
    "DISC_MODEL = Path(pre+\"burningcrusade_best_mmc_yet.pkl\")\n",
    "BURNING_CRUSADE_MODEL = Path(pre+\"burningcrusade_all_green_fnc.pkl\")\n",
    "WOTLK_MODEL = Path(pre+\"burningcrusade_another_fnc.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "QURTYN_MODEL = Path(pre+\"burningcrusade_green_fnc_sharpe_drawdown.pkl\")\n",
    "BOOMKINN_MODEL = Path(pre+\"burningcrusade_4_greenies.pkl\")\n",
    "COMBUSTON_MODEL = Path(pre+\"burningcrusade_3_greenies.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submissions are scored by spearman correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(predictions, targets):\n",
    "    ranked_preds = predictions.rank(pct=True, method=\"first\")\n",
    "    return np.corrcoef(ranked_preds, targets)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convenience method for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(df):\n",
    "    return correlation(df[PREDICTION_NAME], df[TARGET_NAME])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Payout is just the score cliped at +/-25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def payout(scores):\n",
    "    return scores.clip(lower=-0.25, upper=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the csv file into a pandas Dataframe as float16 to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        column_names = next(csv.reader(f))\n",
    "    dtypes = {x: np.float16 for x in column_names if x.startswith(('feature', 'target'))}\n",
    "    df = pd.read_csv(file_path, dtype=dtypes, index_col=0)\n",
    "\n",
    "    # Memory constrained? Try this instead (slower, but more memory efficient)\n",
    "    # see https://forum.numer.ai/t/saving-memory-with-uint8-features/254\n",
    "    # dtypes = {f\"target\": np.float16}\n",
    "    # to_uint8 = lambda x: np.uint8(float(x) * 4)\n",
    "    # converters = {x: to_uint8 for x in column_names if x.startswith('feature')}\n",
    "    # df = pd.read_csv(file_path, dtype=dtypes, converters=converters)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(df,\n",
    "               columns,\n",
    "               extra_neutralizers=None,\n",
    "               proportion=1.0,\n",
    "               normalize=True,\n",
    "               era_col=\"era\"):\n",
    "    # need to do this for lint to be happy bc [] is a \"dangerous argument\"\n",
    "    if extra_neutralizers is None:\n",
    "        extra_neutralizers = []\n",
    "    unique_eras = df[era_col].unique()\n",
    "    computed = []\n",
    "    for u in unique_eras:\n",
    "        print(u, end=\"\\r\")\n",
    "        df_era = df[df[era_col] == u]\n",
    "        scores = df_era[columns].values\n",
    "        if normalize:\n",
    "            scores2 = []\n",
    "            for x in scores.T:\n",
    "                x = (pd.Series(x).rank(method=\"first\").values - .5) / len(x)\n",
    "                scores2.append(x)\n",
    "            scores = np.array(scores2).T\n",
    "            extra = df_era[extra_neutralizers].values\n",
    "            exposures = np.concatenate([extra], axis=1)\n",
    "        else:\n",
    "            exposures = df_era[extra_neutralizers].values\n",
    "        scores -= proportion * exposures.dot(\n",
    "            np.linalg.pinv(exposures.astype(np.float32)).dot(scores.astype(np.float32)))\n",
    "        scores /= scores.std(ddof=0)\n",
    "        computed.append(scores)\n",
    "    return pd.DataFrame(np.concatenate(computed),\n",
    "                        columns=columns,\n",
    "                        index=df.index)\n",
    "def neutralize_series(series, by, proportion=1.0):\n",
    "    scores = series.values.reshape(-1, 1)\n",
    "    exposures = by.values.reshape(-1, 1)\n",
    "\n",
    "    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
    "    exposures = np.hstack(\n",
    "        (exposures,\n",
    "         np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n",
    "    correction = proportion * (exposures.dot(\n",
    "        np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n",
    "    corrected_scores = scores - correction\n",
    "    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
    "    return neutralized\n",
    "def unif(df):\n",
    "    x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
    "    return pd.Series(x, index=df.index)\n",
    "def get_feature_neutral_mean(df, feature_cols):\n",
    "    df.loc[:, \"neutral_sub\"] = neutralize(df, [PREDICTION_NAME],\n",
    "                                          feature_cols)[PREDICTION_NAME]\n",
    "    scores = df.groupby(\"era\").apply(\n",
    "        lambda x: correlation(x[\"neutral_sub\"], x[TARGET_NAME])).mean()\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading new data for round: 263!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianbroeking/projects/numerai/data/numerai_dataset_263.zip:  99%|█████████▉| 400M/403M [00:09<00:00, 38.6MB/s] 2021-05-08 23:16:37,783 INFO numerapi.base_api: unzipping file...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianbroeking/projects/numerai/data/numerai_dataset_263.zip: 403MB [00:25, 15.7MB/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 328 features\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "# The training data is used to train your model how to predict the targets.\n",
    "train, tournament = prepare_data(pre+\"burningcrusade_features.pkl\")\n",
    "# The tournament data is the data that Numerai uses to evaluate your model.\n",
    "feature_names = generate_features_list(train)\n",
    "print(f\"Loaded {len(feature_names)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qurty = load_model(QURTY_MODEL)\n",
    "boomkin = load_model(BOOMKIN_MODEL)\n",
    "disc = load_model(DISC_MODEL)\n",
    "bc = load_model(BURNING_CRUSADE_MODEL)\n",
    "wotlk = load_model(WOTLK_MODEL)\n",
    "qurtyn = load_model(QURTYN_MODEL)\n",
    "boomkinn = load_model(BOOMKINN_MODEL)\n",
    "combustion = load_model(COMBUSTON_MODEL)\n",
    "current_round = NAPI.get_current_round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('qurty', qurty),\n",
    "    ('boomkin', boomkin),\n",
    "    ('disc', disc),\n",
    "    ('bc', bc),\n",
    "    ('wotlk', wotlk),\n",
    "    ('boomkinn', boomkinn),\n",
    "    ('qurtyn', qurtyn),\n",
    "    ('combustion', combustion),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(name, model):\n",
    "    training_data = train.copy()\n",
    "    tournament_data = tournament.copy()\n",
    "    \n",
    "    X_train, y_train = clean_for_xgboost(train)\n",
    "    X_tournament, y_tournament = clean_for_xgboost(tournament)\n",
    "    print(\"Generating predictions...\")\n",
    "    training_data.loc[:, PREDICTION_NAME] = model.predict(X_train)\n",
    "    tournament_data.loc[:, PREDICTION_NAME] = model.predict(X_tournament)\n",
    "\n",
    "    # Check the per-era correlations on the training set (in sample)\n",
    "    train_correlations = training_data.groupby(\"era\").apply(score)\n",
    "    print(f\"On training the correlation has mean {train_correlations.mean()} and std {train_correlations.std(ddof=0)}\")\n",
    "    print(f\"On training the average per-era payout is {payout(train_correlations).mean()}\")\n",
    "    \"\"\"Validation Metrics\"\"\"\n",
    "    # Check the per-era correlations on the validation set (out of sample)\n",
    "    validation_data = tournament_data[tournament_data.data_type == \"validation\"]\n",
    "    validation_correlations = validation_data.groupby(\"era\").apply(score)\n",
    "    print(f\"On validation the correlation has mean {validation_correlations.mean()} and \"\n",
    "        f\"std {validation_correlations.std(ddof=0)}\")\n",
    "    print(f\"On validation the average per-era payout is {payout(validation_correlations).mean()}\")\n",
    "\n",
    "    # Check the \"sharpe\" ratio on the validation set\n",
    "    validation_sharpe = validation_correlations.mean() / validation_correlations.std(ddof=0)\n",
    "    print(f\"Validation Sharpe: {validation_sharpe}\")\n",
    "    print(\"checking max drawdown...\")\n",
    "    rolling_max = (validation_correlations + 1).cumprod().rolling(window=100,\n",
    "                                                                min_periods=1).max()\n",
    "    daily_value = (validation_correlations + 1).cumprod()\n",
    "    max_drawdown = -((rolling_max - daily_value) / rolling_max).max()\n",
    "    print(f\"max drawdown: {max_drawdown}\")\n",
    "\n",
    "    # Check the feature exposure of your validation predictions\n",
    "    feature_exposures = validation_data[feature_names].apply(lambda d: correlation(validation_data[PREDICTION_NAME], d),\n",
    "                                                            axis=0)\n",
    "    max_per_era = validation_data.groupby(\"era\").apply(\n",
    "        lambda d: d[feature_names].corrwith(d[PREDICTION_NAME]).abs().max())\n",
    "    max_feature_exposure = max_per_era.mean()\n",
    "    print(f\"Max Feature Exposure: {max_feature_exposure}\")\n",
    "\n",
    "    # Check feature neutral mean\n",
    "    print(\"Calculating feature neutral mean...\")\n",
    "    feature_neutral_mean = get_feature_neutral_mean(validation_data, X_train.columns)\n",
    "    print(f\"Feature Neutral Mean is {feature_neutral_mean}\")\n",
    "\n",
    "    # Load example preds to get MMC metrics\n",
    "    example_preds = pd.read_csv(f\"../data/numerai_dataset_{current_round}/example_predictions.csv\").set_index(\"id\")[\"prediction\"]\n",
    "    validation_example_preds = example_preds[validation_data.index].values\n",
    "    validation_data.loc[:, \"ExamplePreds\"] = validation_example_preds\n",
    "    print(\"calculating MMC stats...\")\n",
    "    # MMC over validation\n",
    "    mmc_scores = []\n",
    "    corr_scores = []\n",
    "    for _, x in validation_data.groupby(\"era\"):\n",
    "        series = neutralize_series(pd.Series(unif(x[PREDICTION_NAME])),\n",
    "                                pd.Series(unif(x[\"ExamplePreds\"])))\n",
    "        mmc_scores.append(np.cov(series, x[TARGET_NAME])[0, 1] / (0.29 ** 2))\n",
    "        corr_scores.append(correlation(unif(x[PREDICTION_NAME]), x[TARGET_NAME]))\n",
    "    val_mmc_mean = np.mean(mmc_scores)\n",
    "    val_mmc_std = np.std(mmc_scores)\n",
    "    val_mmc_sharpe = val_mmc_mean / val_mmc_std\n",
    "    corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
    "    corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
    "    corr_plus_mmc_mean = np.mean(corr_plus_mmcs)\n",
    "    corr_plus_mmc_sharpe_diff = corr_plus_mmc_sharpe - validation_sharpe\n",
    "    print(\n",
    "        f\"MMC Mean: {val_mmc_mean}\\n\"\n",
    "        f\"Corr Plus MMC Sharpe:{corr_plus_mmc_sharpe}\\n\"\n",
    "        f\"Corr Plus MMC Diff:{corr_plus_mmc_sharpe_diff}\"\n",
    "    )\n",
    "    # Check correlation with example predictions\n",
    "    full_df = pd.concat([pd.DataFrame(validation_example_preds), validation_data[PREDICTION_NAME], validation_data[\"era\"]], axis=1)\n",
    "    full_df.columns = [\"example_preds\", \"prediction\", \"era\"]\n",
    "    per_era_corrs = full_df.groupby('era').apply(lambda d: correlation(unif(d[\"prediction\"]), unif(d[\"example_preds\"])))\n",
    "    corr_with_example_preds = per_era_corrs.mean()\n",
    "    print(f\"Corr with example preds: {corr_with_example_preds}\")\n",
    "\n",
    "    # Save predictions as a CSV and upload to https://numer.ai\n",
    "    tournament_data.set_index('id', inplace=True)\n",
    "    tournament_data[PREDICTION_NAME].to_csv(f\"../submissions/{name}/submission_{name}_{current_round}.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "On training the correlation has mean 0.07938842721104011 and std 0.020025013213848798\n",
      "On training the average per-era payout is 0.07938842721104011\n",
      "On validation the correlation has mean 0.021026576936866777 and std 0.020088435194400896\n",
      "On validation the average per-era payout is 0.021026576936866777\n",
      "Validation Sharpe: 1.0467005883428573\n",
      "checking max drawdown...\n",
      "max drawdown: -0.012104731584593315\n",
      "Max Feature Exposure: 0.3044719324670308\n",
      "Calculating feature neutral mean...\n",
      "era212\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.8/site-packages/pandas/core/indexing.py:1596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.8/site-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Neutral Mean is 0.0006159097986371161\n",
      "calculating MMC stats...\n",
      "MMC Mean: 0.00479077681890712\n",
      "Corr Plus MMC Sharpe:0.9253847481111769\n",
      "Corr Plus MMC Diff:-0.1213158402316804\n",
      "Corr with example preds: 0.5570093484586577\n",
      "Generating predictions...\n",
      "On training the correlation has mean 0.0794471225792282 and std 0.0232163940281972\n",
      "On training the average per-era payout is 0.0794471225792282\n",
      "On validation the correlation has mean 0.020529590120606716 and std 0.01990772611081819\n",
      "On validation the average per-era payout is 0.020529590120606716\n",
      "Validation Sharpe: 1.0312373199393474\n",
      "checking max drawdown...\n",
      "max drawdown: -0.01653109463678609\n",
      "Max Feature Exposure: 0.24562651389898152\n",
      "Calculating feature neutral mean...\n",
      "Feature Neutral Mean is 0.0036643654309945423\n",
      "calculating MMC stats...\n",
      "MMC Mean: 0.002770026241170867\n",
      "Corr Plus MMC Sharpe:0.8951028186438453\n",
      "Corr Plus MMC Diff:-0.13613450129550209\n",
      "Corr with example preds: 0.6326342232248564\n",
      "Generating predictions...\n",
      "On training the correlation has mean 0.07837133339729174 and std 0.019495055184997216\n",
      "On training the average per-era payout is 0.07837133339729174\n",
      "On validation the correlation has mean 0.020066806488190526 and std 0.020835166285094495\n",
      "On validation the average per-era payout is 0.020066806488190526\n",
      "Validation Sharpe: 0.9631219743394295\n",
      "checking max drawdown...\n",
      "max drawdown: -0.01246554459600039\n",
      "Max Feature Exposure: 0.28116326599243274\n",
      "Calculating feature neutral mean...\n",
      "Feature Neutral Mean is 0.0022003698861019654\n",
      "calculating MMC stats...\n",
      "MMC Mean: 0.0039009835181275936\n",
      "Corr Plus MMC Sharpe:0.8192659685400074\n",
      "Corr Plus MMC Diff:-0.14385600579942204\n",
      "Corr with example preds: 0.5616023174202268\n",
      "Generating predictions...\n",
      "On training the correlation has mean 0.0730403129883531 and std 0.022343231060323602\n",
      "On training the average per-era payout is 0.0730403129883531\n",
      "On validation the correlation has mean 0.020898033706744885 and std 0.018713692688807414\n",
      "On validation the average per-era payout is 0.020898033706744885\n",
      "Validation Sharpe: 1.1167242112105387\n",
      "checking max drawdown...\n",
      "max drawdown: -0.020614601925534746\n",
      "Max Feature Exposure: 0.2944855140882317\n",
      "Calculating feature neutral mean...\n",
      "Feature Neutral Mean is 0.007638425792349394\n",
      "calculating MMC stats...\n",
      "MMC Mean: 0.0032949724080367743\n",
      "Corr Plus MMC Sharpe:0.9650837001000596\n",
      "Corr Plus MMC Diff:-0.15164051111047905\n",
      "Corr with example preds: 0.6219547629737938\n",
      "Generating predictions...\n",
      "On training the correlation has mean 0.08203889549333239 and std 0.02083858707695996\n",
      "On training the average per-era payout is 0.08203889549333239\n",
      "On validation the correlation has mean 0.02188342885823641 and std 0.021930911653012877\n",
      "On validation the average per-era payout is 0.02188342885823641\n",
      "Validation Sharpe: 0.9978348918855845\n",
      "checking max drawdown...\n",
      "max drawdown: -0.012557810429201089\n",
      "Max Feature Exposure: 0.26669584529710283\n",
      "Calculating feature neutral mean...\n",
      "Feature Neutral Mean is -0.0013304081422058247\n",
      "calculating MMC stats...\n",
      "MMC Mean: 0.004879368098638205\n",
      "Corr Plus MMC Sharpe:0.8959283807035874\n",
      "Corr Plus MMC Diff:-0.10190651118199712\n",
      "Corr with example preds: 0.5825052141928072\n",
      "Generating predictions...\n",
      "On training the correlation has mean 0.07730740922034184 and std 0.01912289004629519\n",
      "On training the average per-era payout is 0.07730740922034184\n",
      "On validation the correlation has mean 0.02040984820378922 and std 0.018490366629978033\n",
      "On validation the average per-era payout is 0.02040984820378922\n",
      "Validation Sharpe: 1.103809816875084\n",
      "checking max drawdown...\n",
      "max drawdown: -0.01608987974595571\n",
      "Max Feature Exposure: 0.3166830769570132\n",
      "Calculating feature neutral mean...\n",
      "Feature Neutral Mean is 0.00666201085933481\n",
      "calculating MMC stats...\n",
      "MMC Mean: 0.003872938211408448\n",
      "Corr Plus MMC Sharpe:0.9202003214125086\n",
      "Corr Plus MMC Diff:-0.18360949546257543\n",
      "Corr with example preds: 0.576704471273754\n",
      "Generating predictions...\n",
      "On training the correlation has mean 0.06990158731779343 and std 0.018650662517495108\n",
      "On training the average per-era payout is 0.06990158731779343\n",
      "On validation the correlation has mean 0.019512538826946677 and std 0.01775753427723464\n",
      "On validation the average per-era payout is 0.019512538826946677\n",
      "Validation Sharpe: 1.0988315450958737\n",
      "checking max drawdown...\n",
      "max drawdown: -0.016802327695347376\n",
      "Max Feature Exposure: 0.3892473405752012\n",
      "Calculating feature neutral mean...\n",
      "Feature Neutral Mean is 0.005293603855123298\n",
      "calculating MMC stats...\n",
      "MMC Mean: 0.0037032720546738445\n",
      "Corr Plus MMC Sharpe:0.9006472152324314\n",
      "Corr Plus MMC Diff:-0.19818432986344225\n",
      "Corr with example preds: 0.5524787105813945\n",
      "Generating predictions...\n",
      "On training the correlation has mean 0.07633855338915542 and std 0.019270342071812364\n",
      "On training the average per-era payout is 0.07633855338915542\n",
      "On validation the correlation has mean 0.01959835758298786 and std 0.018676114125305714\n",
      "On validation the average per-era payout is 0.01959835758298786\n",
      "Validation Sharpe: 1.0493809071573688\n",
      "checking max drawdown...\n",
      "max drawdown: -0.0168860776153242\n",
      "Max Feature Exposure: 0.34411866137127817\n",
      "Calculating feature neutral mean...\n",
      "Feature Neutral Mean is 0.007784052972543004\n",
      "calculating MMC stats...\n",
      "MMC Mean: 0.0036679035481927574\n",
      "Corr Plus MMC Sharpe:0.8761509137243755\n",
      "Corr Plus MMC Diff:-0.17322999343299328\n",
      "Corr with example preds: 0.5563186665152006\n"
     ]
    }
   ],
   "source": [
    "for pair in models:\n",
    "    generate(pair[0], pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
