{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "decent-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix, mean_absolute_error\n",
    "\n",
    "import xgboost\n",
    "import numerapi\n",
    "NAPI = numerapi.NumerAPI(verbosity=\"info\")\n",
    "import random as rn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from bayes_opt import BayesianOptimization\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from helpers.utils import *\n",
    "from helpers.xgboost_feval import *\n",
    "from models.burningcrusade.data_preparation import prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "medium-arrangement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You already have the newest data! Current round is: 261\n",
      "Loading the data\n"
     ]
    }
   ],
   "source": [
    "DIR = \"/Users/brianbroeking/projects/numerai/data\"\n",
    "download_current_data(DIR)\n",
    "train_static, val_static, tournament_static = load_data(DIR, reduce_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "suspended-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_group = get_group_stats(train_static)\n",
    "tournament_with_group = get_group_stats(tournament_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "compatible-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_NAME = f\"target\"\n",
    "PREDICTION_NAME = f\"prediction\"\n",
    "feature_names = generate_features_list(train_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "global-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_corr_list = random.sample(feature_names, 31)\n",
    "train, tournament = generate_polynomial_features(ft_corr_list, train_with_group, tournament_with_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "offensive-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = clean_for_xgboost(train)\n",
    "dtrain = xgboost.DMatrix(X_train, y_train)\n",
    "\n",
    "X_tournament, y_tournament = clean_for_xgboost(tournament)\n",
    "dtournament = xgboost.DMatrix(X_tournament, y_tournament)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-stranger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Era boost train with proportion 0.500 ####\n",
      "\n",
      "Train 20 iterations\n",
      "Train 10 rounds per iteration\n",
      "\n",
      "Iteration 1:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ar1(x):\n",
    "    return np.corrcoef(x[:-1], x[1:])[0,1]\n",
    "\n",
    "def autocorr_penalty(x):\n",
    "    n = len(x)\n",
    "    p = ar1(x)\n",
    "    return np.sqrt(1 + 2*np.sum([((n - i)/n)*p**i for i in range(1,n)]))\n",
    "\n",
    "def smart_sharpe(x):\n",
    "    return np.mean(x)/(np.std(x, ddof=1)*autocorr_penalty(x))\n",
    "\n",
    "def era_boost_train(X, y, era_col, proportion=0.5,\n",
    "                    trees_per_step=10, num_iters=200,\n",
    "                    one_shot=False, tree_method='hist',\n",
    "                    test_model=None, note=None):\n",
    "    print(f\"\\n#### Era boost train with proportion {proportion:0.3f} ####\\n\")\n",
    "    if note is not None:\n",
    "        print(note)\n",
    "    if one_shot:\n",
    "        trees_per_step = trees_per_step * num_iters\n",
    "        num_iters=1\n",
    "\n",
    "    if test_model is None:\n",
    "        print(f\"Train {num_iters} iterations\")\n",
    "        print(f\"Train {trees_per_step} rounds per iteration\")\n",
    "    else:\n",
    "        print(\"Testing model performance\")\n",
    "    features = X.columns\n",
    "    new_df = X.copy()\n",
    "    new_df[\"target\"] = y\n",
    "    new_df[\"era\"] = era_col\n",
    "    for i in range(num_iters):\n",
    "        print(f\"\\nIteration {i+1}:\\n\")\n",
    "        if test_model is None:\n",
    "            if i==0:\n",
    "                model = xgboost.XGBRegressor(max_depth=18,\n",
    "                                             learning_rate=0.001,\n",
    "                                             n_estimators=trees_per_step,\n",
    "                                             n_jobs=-1,\n",
    "                                             colsample_bytree=0.1,\n",
    "                                             gamma=0.1,\n",
    "                                             tree_method=tree_method)\n",
    "                model.fit(X, y)\n",
    "            else:\n",
    "                model.n_estimators += trees_per_step\n",
    "                booster = model.get_booster()\n",
    "                print(\"fitting on worst eras\")\n",
    "                model.fit(worst_df[features], worst_df[\"target\"], xgb_model=booster)\n",
    "        else:\n",
    "            model = test_model\n",
    "        # score each era\n",
    "        print(\"predicting on train\")\n",
    "        preds = model.predict(X)\n",
    "        new_df[\"pred\"] = preds\n",
    "        era_scores = pd.Series(index=new_df[\"era\"].unique())\n",
    "        print(\"getting per era scores\")\n",
    "        for era in new_df[\"era\"].unique():\n",
    "            era_df = new_df[new_df[\"era\"] == era]\n",
    "            print(spearmanr(era_df[\"pred\"], era_df[\"target\"])[0])\n",
    "            era_scores[era] = spearmanr(era_df[\"pred\"], era_df[\"target\"])[0]\n",
    "        era_scores.sort_values(inplace=True)\n",
    "        worst_eras = era_scores[era_scores <= era_scores.quantile(proportion)].index\n",
    "        print(list(worst_eras))\n",
    "        worst_df = new_df[new_df[\"era\"].isin(worst_eras)]\n",
    "        era_scores.sort_index(inplace=True)\n",
    "        era_scores.plot(kind=\"bar\")\n",
    "        print(\"performance over time\")\n",
    "        plt.show()\n",
    "        print(\"autocorrelation\")\n",
    "        print(ar1(era_scores))\n",
    "        if (ar1(era_scores)) < 0.1:\n",
    "            return model\n",
    "        print(\"mean correlation\")\n",
    "        print(np.mean(era_scores))\n",
    "        print(\"sharpe\")\n",
    "        print(np.mean(era_scores)/np.std(era_scores))\n",
    "        print(\"smart sharpe\")\n",
    "        print(smart_sharpe(era_scores))\n",
    "    return model\n",
    "\n",
    "boost_model = era_boost_train(X_train, y_train,\n",
    "                              era_col=train[\"era\"], proportion=0.5,\n",
    "                              trees_per_step=10, num_iters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(boost_model, \"burningcrusade.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-screen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "tournament.loc[:, PREDICTION_NAME] = boost_model.predict(X_tournament)\n",
    "current_round = NAPI.get_current_round()\n",
    "tournament.set_index('id', inplace=True)\n",
    "tournament[PREDICTION_NAME].to_csv(f\"submissions/burningcrusade/submission_{current_round}.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-amendment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
