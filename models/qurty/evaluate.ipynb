{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExample classifier on Numerai data using a xgboost regression.\\nTo get started, install the required packages: pip install pandas numpy sklearn xgboost\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Example classifier on Numerai data using a xgboost regression.\n",
    "To get started, install the required packages: pip install pandas numpy sklearn xgboost\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from models.qurty.data_preparation import prepare_data\n",
    "from helpers.utils import generate_features_list, load_model, clean_for_xgboost\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numerapi\n",
    "NAPI = numerapi.NumerAPI(verbosity=\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_NAME = f\"target\"\n",
    "PREDICTION_NAME = f\"prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILE = Path(\"model_qurty.pickle.dat\")\n",
    "ERA_BOOSTED_MODEL_FILE = Path(\"era_boosted_qurty.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submissions are scored by spearman correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(predictions, targets):\n",
    "    ranked_preds = predictions.rank(pct=True, method=\"first\")\n",
    "    return np.corrcoef(ranked_preds, targets)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convenience method for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(df):\n",
    "    return correlation(df[PREDICTION_NAME], df[TARGET_NAME])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Payout is just the score cliped at +/-25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def payout(scores):\n",
    "    return scores.clip(lower=-0.25, upper=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the csv file into a pandas Dataframe as float16 to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        column_names = next(csv.reader(f))\n",
    "    dtypes = {x: np.float16 for x in column_names if x.startswith(('feature', 'target'))}\n",
    "    df = pd.read_csv(file_path, dtype=dtypes, index_col=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(training_data, tournament_data):\n",
    "    train_correlations = training_data.groupby(\"era\").apply(score)\n",
    "    print(f\"On training the correlation has mean {train_correlations.mean()} and std {train_correlations.std(ddof=0)}\")\n",
    "    print(f\"On training the average per-era payout is {payout(train_correlations).mean()}\")\n",
    "    \"\"\"Validation Metrics\"\"\"\n",
    "    # Check the per-era correlations on the validation set (out of sample)\n",
    "    validation_data = tournament_data[tournament_data.data_type == \"validation\"]\n",
    "    validation_correlations = validation_data.groupby(\"era\").apply(score)\n",
    "    print(f\"On validation the correlation has mean {validation_correlations.mean()} and \"\n",
    "        f\"std {validation_correlations.std(ddof=0)}\")\n",
    "    print(f\"On validation the average per-era payout is {payout(validation_correlations).mean()}\")\n",
    "\n",
    "    # Check the \"sharpe\" ratio on the validation set\n",
    "    validation_sharpe = validation_correlations.mean() / validation_correlations.std(ddof=0)\n",
    "    print(f\"Validation Sharpe: {validation_sharpe}\")\n",
    "    print(\"checking max drawdown...\")\n",
    "    rolling_max = (validation_correlations + 1).cumprod().rolling(window=100,\n",
    "                                                                min_periods=1).max()\n",
    "    daily_value = (validation_correlations + 1).cumprod()\n",
    "    max_drawdown = -((rolling_max - daily_value) / rolling_max).max()\n",
    "    print(f\"max drawdown: {max_drawdown}\")\n",
    "\n",
    "    # Check the feature exposure of your validation predictions\n",
    "    feature_exposures = validation_data[feature_names].apply(lambda d: correlation(validation_data[PREDICTION_NAME], d),\n",
    "                                                            axis=0)\n",
    "    max_per_era = validation_data.groupby(\"era\").apply(\n",
    "        lambda d: d[feature_names].corrwith(d[PREDICTION_NAME]).abs().max())\n",
    "    max_feature_exposure = max_per_era.mean()\n",
    "    print(f\"Max Feature Exposure: {max_feature_exposure}\")\n",
    "\n",
    "    # Check feature neutral mean\n",
    "    print(\"Calculating feature neutral mean...\")\n",
    "    feature_neutral_mean = get_feature_neutral_mean(validation_data, X_train.columns)\n",
    "    print(f\"Feature Neutral Mean is {feature_neutral_mean}\")\n",
    "\n",
    "    # Load example preds to get MMC metrics\n",
    "    example_preds = pd.read_csv(f\"../../data/numerai_dataset_{current_round}/example_predictions.csv\").set_index(\"id\")[\"prediction\"]\n",
    "    validation_example_preds = example_preds[validation_data.index].values\n",
    "    validation_data.loc[:, \"ExamplePreds\"] = validation_example_preds\n",
    "    print(\"calculating MMC stats...\")\n",
    "    # MMC over validation\n",
    "    mmc_scores = []\n",
    "    corr_scores = []\n",
    "    for _, x in validation_data.groupby(\"era\"):\n",
    "        series = neutralize_series(pd.Series(unif(x[PREDICTION_NAME])),\n",
    "                                pd.Series(unif(x[\"ExamplePreds\"])))\n",
    "        mmc_scores.append(np.cov(series, x[TARGET_NAME])[0, 1] / (0.29 ** 2))\n",
    "        corr_scores.append(correlation(unif(x[PREDICTION_NAME]), x[TARGET_NAME]))\n",
    "    val_mmc_mean = np.mean(mmc_scores)\n",
    "    val_mmc_std = np.std(mmc_scores)\n",
    "    val_mmc_sharpe = val_mmc_mean / val_mmc_std\n",
    "    corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
    "    corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
    "    corr_plus_mmc_mean = np.mean(corr_plus_mmcs)\n",
    "    corr_plus_mmc_sharpe_diff = corr_plus_mmc_sharpe - validation_sharpe\n",
    "    print(\n",
    "        f\"MMC Mean: {val_mmc_mean}\\n\"\n",
    "        f\"Corr Plus MMC Sharpe:{corr_plus_mmc_sharpe}\\n\"\n",
    "        f\"Corr Plus MMC Diff:{corr_plus_mmc_sharpe_diff}\"\n",
    "    )\n",
    "    # Check correlation with example predictions\n",
    "    full_df = pd.concat([pd.DataFrame(validation_example_preds), validation_data[PREDICTION_NAME], validation_data[\"era\"]], axis=1)\n",
    "    full_df.columns = [\"example_preds\", \"prediction\", \"era\"]\n",
    "    per_era_corrs = full_df.groupby('era').apply(lambda d: correlation(unif(d[\"prediction\"]), unif(d[\"example_preds\"])))\n",
    "    corr_with_example_preds = per_era_corrs.mean()\n",
    "    print(f\"Corr with example preds: {corr_with_example_preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "You already have the newest data! Current round is: 261\n",
      "Loading the data\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "# The training data is used to train your model how to predict the targets.\n",
    "training_data, tournament_data = prepare_data(ft_corr_list='qurty_ft_corr_list.pickle.dat')\n",
    "# The tournament data is the data that Numerai uses to evaluate your model.\n",
    "feature_names = generate_features_list(training_data)\n",
    "print(f\"Loaded {len(feature_names)} features\")\n",
    "model = load_model(MODEL_FILE)\n",
    "era_boosted_model = load_model(ERA_BOOSTED_MODEL_FILE)\n",
    "current_round = NAPI.get_current_round()\n",
    "\n",
    "# Generate predictions on both training and tournament data\n",
    "print(\"Generating predictions...\")\n",
    "X_train, y_train = clean_for_xgboost(training_data)\n",
    "X_test, y_test = clean_for_xgboost(tournament_data)\n",
    "\n",
    "dtrain = xgboost.DMatrix(X_train, y_train)\n",
    "dtest = xgboost.DMatrix(X_test, y_test)\n",
    "\n",
    "training_data.loc[:, PREDICTION_NAME] = model.predict(dtrain)\n",
    "tournament_data.loc[:, PREDICTION_NAME] = model.predict(dtest)\n",
    "era_boost_training = training_data[['id']].copy()\n",
    "era_boost_tournament = tournament_data[['id']].copy()\n",
    "era_boost_training.loc[:, PREDICTION_NAME] = era_boosted_model.predict(X_train)\n",
    "era_boost_tournament.loc[:, PREDICTION_NAME] = era_boosted_model.predict(X_test)\n",
    "\n",
    "# Check the per-era correlations on the training set (in sample)\n",
    "print(\"Generating Metrics\")\n",
    "evaluate(training_data, tournament_data)\n",
    "evaluate(era_boost_training, era_boost_tournament)\n",
    "\n",
    "# Save predictions as a CSV and upload to https://numer.ai\n",
    "tournament_data.set_index('id', inplace=True)\n",
    "tournament_data[PREDICTION_NAME].to_csv(f\"../../submissions/qurty/submission_{current_round}.csv\", header=True)\n",
    "\n",
    "# Era boosted \n",
    "era_boosted_tournament.set_index('id', inplace=True)\n",
    "era_boosted_tournament[PREDICTION_NAME].to_csv(f\"../../submissions/qurty/era_boosted_submission_{current_round}.csv\", header=['prediction'])\n",
    "\n",
    "# Neutralize\n",
    "neutralized = tournament_data.copy()\n",
    "neutralized.loc[:, PREDICTION_NAME] = neutralize_series(tournament_data[PREDICTION_NAME], \n",
    "                                                    example_preds, 0.25)\n",
    "neutralized[PREDICTION_NAME].to_csv(f\"../../submissions/qurty/submission_{current_round}_neutralized.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " <br>\n",
    "functions used for advanced metrics<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to neutralize a column in a df by many other columns on a per-era basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(df,\n",
    "               columns,\n",
    "               extra_neutralizers=None,\n",
    "               proportion=1.0,\n",
    "               normalize=True,\n",
    "               era_col=\"era\"):\n",
    "    # need to do this for lint to be happy bc [] is a \"dangerous argument\"\n",
    "    if extra_neutralizers is None:\n",
    "        extra_neutralizers = []\n",
    "    unique_eras = df[era_col].unique()\n",
    "    computed = []\n",
    "    for u in unique_eras:\n",
    "        print(u, end=\"\\r\")\n",
    "        df_era = df[df[era_col] == u]\n",
    "        scores = df_era[columns].values\n",
    "        if normalize:\n",
    "            scores2 = []\n",
    "            for x in scores.T:\n",
    "                x = (pd.Series(x).rank(method=\"first\").values - .5) / len(x)\n",
    "                scores2.append(x)\n",
    "            scores = np.array(scores2).T\n",
    "            extra = df_era[extra_neutralizers].values\n",
    "            exposures = np.concatenate([extra], axis=1)\n",
    "        else:\n",
    "            exposures = df_era[extra_neutralizers].values\n",
    "        scores -= proportion * exposures.dot(\n",
    "            np.linalg.pinv(exposures.astype(np.float32)).dot(scores.astype(np.float32)))\n",
    "        scores /= scores.std(ddof=0)\n",
    "        computed.append(scores)\n",
    "    return pd.DataFrame(np.concatenate(computed),\n",
    "                        columns=columns,\n",
    "                        index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to neutralize any series by any other series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize_series(series, by, proportion=1.0):\n",
    "    scores = series.values.reshape(-1, 1)\n",
    "    exposures = by.values.reshape(-1, 1)\n",
    "\n",
    "    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
    "    exposures = np.hstack(\n",
    "        (exposures,\n",
    "         np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n",
    "    correction = proportion * (exposures.dot(\n",
    "        np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n",
    "    corrected_scores = scores - correction\n",
    "    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
    "    return neutralized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unif(df):\n",
    "    x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
    "    return pd.Series(x, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_neutral_mean(df, feature_cols):\n",
    "    df.loc[:, \"neutral_sub\"] = neutralize(df, [PREDICTION_NAME],\n",
    "                                          feature_cols)[PREDICTION_NAME]\n",
    "    scores = df.groupby(\"era\").apply(\n",
    "        lambda x: correlation(x[\"neutral_sub\"], x[TARGET_NAME])).mean()\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
